# Cloudflare Configuration for MCP Memory Service
# ================================================
# Copy this file to .env and replace with your actual credentials
#
# Setup Instructions:
# 1. Copy this file: cp .env.example .env
# 2. Create Cloudflare API Token at: https://dash.cloudflare.com/profile/api-tokens
# 3. Replace placeholder values below with your actual credentials
# 4. Never commit your .env file to git (it's already in .gitignore)

# =============================================================================
# REQUIRED: Cloudflare API Token
# =============================================================================
# Create at: https://dash.cloudflare.com/profile/api-tokens
# Required permissions:
#   - Account: Cloudflare Workers:Edit
#   - Zone Resources: Include All zones
#   - Account Resources: Include All accounts
#
# IMPORTANT: Test your token with the account-scoped endpoint:
#   curl "https://api.cloudflare.com/client/v4/accounts/{ACCOUNT_ID}/tokens/verify" \
#        -H "Authorization: Bearer {YOUR_TOKEN}"
#
# DO NOT use the generic endpoint (will fail for scoped tokens):
#   curl "https://api.cloudflare.com/client/v4/user/tokens/verify" ❌
CLOUDFLARE_API_TOKEN=your-cloudflare-api-token-here

# =============================================================================
# REQUIRED: Cloudflare Account ID
# =============================================================================
# Find in: Cloudflare Dashboard > Right sidebar under "Account ID"
# Example: be0e35a26715043ef8df90253268c33f
CLOUDFLARE_ACCOUNT_ID=your-account-id-here

# =============================================================================
# REQUIRED: D1 Database ID
# =============================================================================
# Create with: wrangler d1 create mcp-memory-database
# Or find existing: wrangler d1 list
# Example: f745e9b4-ba8e-4d47-b38f-12af91060d5a
CLOUDFLARE_D1_DATABASE_ID=your-d1-database-id-here

# =============================================================================
# REQUIRED: Vectorize Index Name
# =============================================================================
# Create with: wrangler vectorize create mcp-memory-index --dimensions=384
# Or find existing: wrangler vectorize list
# Example: mcp-memory-index
CLOUDFLARE_VECTORIZE_INDEX=your-vectorize-index-name

# =============================================================================
# OPTIONAL: R2 Bucket for Large Content Storage
# =============================================================================
# Create with: wrangler r2 bucket create mcp-memory-content
# Only needed if you plan to store large content (>1MB)
# CLOUDFLARE_R2_BUCKET=mcp-memory-content

# =============================================================================
# STORAGE BACKEND CONFIGURATION
# =============================================================================
# Options: sqlite_vec | cloudflare | hybrid
# - sqlite_vec: Fast local storage (development)
# - cloudflare: Cloud storage with Cloudflare (production)
# - hybrid: Best of both - local speed + cloud persistence (recommended)
MCP_MEMORY_STORAGE_BACKEND=cloudflare

# =============================================================================
# OPTIONAL: Advanced Configuration
# =============================================================================

# Semantic deduplication settings
MCP_SEMANTIC_DEDUP_ENABLED=true               # Enable/disable semantic dedup (default: true)
MCP_SEMANTIC_DEDUP_TIME_WINDOW_HOURS=24      # Hours to look back (default: 24)
MCP_SEMANTIC_DEDUP_THRESHOLD=0.85            # Similarity threshold 0.0-1.0 (default: 0.85)

# Cloudflare embedding model (default is recommended)
# CLOUDFLARE_EMBEDDING_MODEL=@cf/baai/bge-base-en-v1.5

# Large content threshold for R2 storage (bytes)
# CLOUDFLARE_LARGE_CONTENT_THRESHOLD=1048576

# HTTP Interface (Web Dashboard)
# NOTE: The HTTP dashboard is a SEPARATE server from the MCP server.
# To start the dashboard: uv run python scripts/server/run_http_server.py
# Or use Windows Task Scheduler: scripts/service/windows/install_scheduled_task.ps1
MCP_HTTP_ENABLED=true
MCP_HTTP_PORT=8000
# MCP_HTTPS_ENABLED=true
# MCP_HTTPS_PORT=8443

# =============================================================================
# Authentication
# =============================================================================

# API Key Authentication (simple, no OAuth required)
# Accepts API key via X-API-Key header or api_key query parameter
# Example: curl -H "X-API-Key: your-secret-key" http://localhost:8000/api/memories
# MCP_API_KEY=your-secret-key-here

# OAuth 2.1 Authentication (for web interface with team collaboration)
# MCP_OAUTH_ENABLED=false

# OAuth Storage Backend (memory|sqlite)
MCP_OAUTH_STORAGE_BACKEND=memory
MCP_OAUTH_SQLITE_PATH=./data/oauth.db

# Anonymous Access (allow unauthenticated read-only access)
# Only enable this if you're behind a firewall or using external auth (e.g., Nginx Basic Auth)
# MCP_ALLOW_ANONYMOUS_ACCESS=false

# Hybrid Backend Configuration (if using hybrid)
# MCP_HYBRID_SYNC_INTERVAL=300      # Sync every 5 minutes
# MCP_HYBRID_BATCH_SIZE=50          # Sync 50 operations at a time
# MCP_HYBRID_SYNC_ON_STARTUP=true   # Initial sync on startup

# Offline Mode (for air-gapped deployments)
# MCP_MEMORY_OFFLINE=1              # Prevent HuggingFace model downloads
                                    # Note: Models must be pre-cached before enabling
                                    # First install downloads models automatically

# =============================================================================
# OPTIONAL: External Embedding API (vLLM, Ollama, TEI, OpenAI)
# =============================================================================
# Use external embedding service instead of local models
# Useful for: shared infrastructure, GPU offloading, hosted services
#
# ⚠️  IMPORTANT: Only supported with MCP_MEMORY_STORAGE_BACKEND=sqlite_vec
# External APIs do NOT work with 'hybrid' or 'cloudflare' backends
#
# API endpoint (OpenAI-compatible /v1/embeddings)
# MCP_EXTERNAL_EMBEDDING_URL=http://localhost:8890/v1/embeddings
#
# Model name to pass to the API
# MCP_EXTERNAL_EMBEDDING_MODEL=nomic-embed-text
#
# Optional: API key for authenticated endpoints
# MCP_EXTERNAL_EMBEDDING_API_KEY=sk-xxx
#
# Examples:
# - vLLM: http://localhost:8890/v1/embeddings
# - Ollama: http://localhost:11434/v1/embeddings
# - OpenAI: https://api.openai.com/v1/embeddings
# - TEI: http://localhost:8080/v1/embeddings
#
# ⚠️  Embedding dimensions must match your storage backend:
#   - all-MiniLM-L6-v2 (default ONNX): 384 dims
#   - nomic-embed-text: 768 dims
#   - text-embedding-3-small: 1536 dims
#   - @cf/baai/bge-base-en-v1.5 (Cloudflare): 768 dims

# =============================================================================
# GRAPH DATABASE CONFIGURATION (v8.51.0+)
# =============================================================================
# Controls how memory associations are stored
# Options:
#   - memories_only: Store in memories.metadata.associations (backward compatible, v8.48.0 behavior)
#   - dual_write: Write to both memories.metadata AND memory_graph table (migration mode, default)
#   - graph_only: Write to memory_graph table only (future mode, requires migration complete)
#
# Migration Path:
#   1. Start with dual_write (default) - maintains compatibility while building graph data
#   2. Run migration script to backfill memory_graph from existing associations
#   3. Validate graph data completeness
#   4. Switch to graph_only mode after validation
#
# Benefits of graph_only mode:
#   - 5-10x faster association queries (indexed graph table vs JSON metadata parsing)
#   - Efficient bidirectional traversal (A→B and B→A)
#   - Support for multiple connection types (semantic, temporal, causal, thematic)
#   - Foundation for advanced graph analytics (PageRank, clustering, path finding)
#
MCP_GRAPH_STORAGE_MODE=dual_write

# =============================================================================
# HYBRID SEARCH CONFIGURATION (v10.8.0+)
# =============================================================================
# Hybrid search combines BM25 keyword matching with vector similarity search
# for better exact match scoring. Only available with sqlite_vec backend.
#
# Enable/disable hybrid search (default: true)
MCP_HYBRID_SEARCH_ENABLED=true

# Score fusion weights (must sum to ~1.0)
# - Keyword weight: BM25 keyword match score weight (default: 0.3)
# - Semantic weight: Vector similarity score weight (default: 0.7)
#
# Adjust weights based on your use case:
# - More keyword-focused: increase keyword_weight (e.g., 0.5/0.5)
# - More semantic-focused: increase semantic_weight (e.g., 0.2/0.8)
MCP_HYBRID_KEYWORD_WEIGHT=0.3
MCP_HYBRID_SEMANTIC_WEIGHT=0.7

# =============================================================================
# TROUBLESHOOTING
# =============================================================================
# Common issues:
# 1. "Invalid API Token" - Check token permissions and expiry
# 2. "Database not found" - Verify D1 database ID is correct
# 3. "Vectorize index not found" - Check index name and dimensions (384)
# 4. "Account access denied" - Ensure API token has account permissions
#
# Documentation: https://github.com/doobidoo/mcp-memory-service/wiki
# Support: https://github.com/doobidoo/mcp-memory-service/issues